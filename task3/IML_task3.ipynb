{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f05125b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b8a6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_function():\n",
    "    train_data = 'train_triplets.txt'\n",
    "    with open(train_data, 'r') as file:\n",
    "        triplets = [line for line in file.readlines()]\n",
    "    train_samples, val_samples = train_test_split(triplets, test_size=0.1)\n",
    "    with open('val_samples.txt', 'w') as file:\n",
    "        for item in val_samples:\n",
    "            file.write(item)\n",
    "    with open('train_samples.txt', 'w') as file:\n",
    "        for item in train_samples:\n",
    "            file.write(item)\n",
    "    return len(train_samples)\n",
    "\n",
    "def compute_distances(outputs):\n",
    "    positive = tf.reduce_sum(tf.square(outputs[..., 0] - outputs[..., 1]), axis=1)\n",
    "    negative = tf.reduce_sum(tf.square(outputs[..., 0] - outputs[..., 2]), axis=1)\n",
    "    return positive, negative\n",
    "\n",
    "def triplet_loss(_, outputs):\n",
    "    positive, negative = compute_distances(outputs)\n",
    "    return tf.reduce_mean(tf.math.softplus(positive - negative))\n",
    "\n",
    "\n",
    "def accuracy(_, outputs):\n",
    "    positive, negative = compute_distances(outputs)\n",
    "    return tf.reduce_mean(tf.cast(tf.greater_equal(negative, positive), tf.float32))\n",
    "\n",
    "\n",
    "def load_triplets(triplet, training):\n",
    "    ids = tf.strings.split(triplet)\n",
    "    image_triplet = []\n",
    "    for i in range(3):\n",
    "        image = tf.io.read_file('food/' + ids[i] + '.jpg')\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = tf.image.resize(image, (224, 224))\n",
    "        image = tf.keras.applications.mobilenet_v3.preprocess_input(image)\n",
    "        image_triplet.append(image)\n",
    "    if training:\n",
    "        return tf.stack(image_triplet, axis=0), 1\n",
    "    else:\n",
    "        return tf.stack(image_triplet, axis=0)\n",
    "\n",
    "def load_dataset(dataset_filename, training=True):\n",
    "    dataset = tf.data.TextLineDataset(dataset_filename)\n",
    "    dataset = dataset.map(lambda triplet: load_triplets(triplet, training), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559a117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1340/1340 [==============================] - 1577s 1s/step - loss: 0.5139 - accuracy: 0.7557 - val_loss: 0.4885 - val_accuracy: 0.7826 - lr: 5.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a37417a60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image_height = 224\n",
    "Image_width = 224\n",
    "epochs = 1\n",
    "train_batch_size = 40\n",
    "train_sample_size = split_function()\n",
    "\n",
    "train_dataset = load_dataset('train_samples.txt')\n",
    "val_dataset = load_dataset('val_samples.txt')\n",
    "\n",
    "pretrained_model = tf.keras.applications.MobileNetV3Large(include_top=False, input_shape=(Image_height, Image_width, 3))\n",
    "pretrained_model.trainable = False\n",
    "custom_layers = tf.keras.Sequential([\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(300),\n",
    "    tf.keras.layers.Lambda(lambda t: tf.math.l2_normalize(t, axis=1))\n",
    "    ]) \n",
    "inputs = tf.keras.Input(shape=(3, Image_height, Image_width, 3))\n",
    "output_triplet = []\n",
    "for i in range(3):\n",
    "    output_triplet.append(custom_layers(pretrained_model(inputs[:, i, ...])))\n",
    "    \n",
    "outputs = tf.stack(output_triplet, axis=-1)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "#model.summary()\n",
    "\n",
    "train_dataset = train_dataset.shuffle(1024, reshuffle_each_iteration=True).repeat().batch(train_batch_size)\n",
    "val_dataset = val_dataset.batch(train_batch_size)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss=triplet_loss, metrics=[accuracy])\n",
    "\n",
    "earlystopping = EarlyStopping(monitor=\"val_accuracy\", mode=\"max\", patience=3, verbose=1)\n",
    "redlrplat = ReduceLROnPlateau(monitor=\"val_accuracy\", mode=\"max\", factor=0.5, patience=1, verbose=2)\n",
    "callbacks_list = [earlystopping, redlrplat] \n",
    "\n",
    "model.fit(train_dataset, steps_per_epoch=int(np.ceil(train_sample_size / train_batch_size)), callbacks = callbacks_list,\n",
    "epochs=epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1db6aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346ba21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "993/993 [==============================] - 1604s 2s/step\n"
     ]
    }
   ],
   "source": [
    "inference_batch_size = 60\n",
    "test_sample_size = 59544\n",
    "test_dataset = load_dataset('test_triplets.txt', training=False).batch(inference_batch_size).prefetch(2)\n",
    "\n",
    "positive, negative = compute_distances(model.output)\n",
    "predictions = tf.cast(tf.greater_equal(negative, positive), tf.int8)\n",
    "test_model = tf.keras.Model(inputs=model.inputs, outputs=predictions)\n",
    "\n",
    "predictions = test_model.predict(test_dataset, steps=int(np.ceil(test_sample_size / inference_batch_size)), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1da4e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predictions.txt', predictions, fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87765ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6883444b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d59c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc94bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g_gpu",
   "language": "python",
   "name": "ml4g_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
